{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from random import randint\n",
    "import libs.utils as custom_utils\n",
    "import libs.net_utils as net_utils\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image  # module\n",
    "\n",
    "# Set torch seed\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "# Initialize training variables\n",
    "BATCH = 16\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a dataset object to use as input on a CNN\n",
    "    \"\"\"\n",
    "    def __init__(self, root):\n",
    "        \"\"\"\n",
    "        Default initializer\n",
    "        :param root: path to dataset root\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.size = custom_utils.IMG_SIZE\n",
    "\n",
    "        # Load images filelist\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # Load annotations filelist\n",
    "        self.annotations = list(sorted(os.listdir(os.path.join(root, \"annotations\"))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Default getter for dataset objects\n",
    "        :param index: i of the wanted image + annotation\n",
    "        :return: image as PIL Image and target dictionary\n",
    "        \"\"\"\n",
    "        img = self.__load_image(index)\n",
    "        target = self.__generate_target(index)\n",
    "        if self.size is not None:\n",
    "            img, target = self.__apply_transform(img, target)\n",
    "\n",
    "        target[\"objectness\"] = self.__compute_objectness(target['boxes'])\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __apply_transform(self, img, target):\n",
    "        \"\"\"\n",
    "        Apply a resize transformation to an image and its target\n",
    "        :param img: image as PIL Image\n",
    "        :param target: dict representing the bounding boxes\n",
    "        \"\"\"\n",
    "        target[\"boxes\"] = self.__resize_boxes(target[\"boxes\"], img.size)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((self.size, self.size))])\n",
    "        img = transform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __resize_boxes(self, boxes, img_size):\n",
    "        \"\"\"\n",
    "        Apply to bounding boxes the same resize as the corresponding image\n",
    "        :param boxes: tensor containing the coordinates of the bounding boxes\n",
    "        :param img_size: size of the original image\n",
    "        \"\"\"\n",
    "        x_scale = self.size/img_size[0]\n",
    "        y_scale = self.size/img_size[1]\n",
    "\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            box = box.tolist()\n",
    "            x = int(np.round(box[0] * x_scale))\n",
    "            y = int(np.round(box[1] * y_scale))\n",
    "            x_max = int(np.round(box[2] * x_scale))\n",
    "            y_max = int(np.round(box[3] * y_scale))\n",
    "            scaled_boxes.append([x, y, x_max, y_max])\n",
    "        return torch.as_tensor(scaled_boxes, dtype=torch.float32, device=custom_utils.DEVICE)\n",
    "\n",
    "    def __load_image(self, index):\n",
    "        \"\"\"\n",
    "        Load an image from the list of available images\n",
    "        :param index: i of the wanted image\n",
    "        :return: the image as a PIL.Image object\n",
    "        \"\"\"\n",
    "        image_path = os.path.join(self.root, \"images\", self.images[index])\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    def __load_annotation(self, index):\n",
    "        \"\"\"\n",
    "        Load image annotations from the list of available annotations files\n",
    "        :param index: i of the wanted image\n",
    "        :return: the annotations as a dict\n",
    "        \"\"\"\n",
    "        annotation_path = os.path.join(self.root, \"annotations\", self.annotations[index])\n",
    "        with open(annotation_path, \"r\") as fp:\n",
    "            annotation_json = json.load(fp)\n",
    "        return [value for key, value in annotation_json.items() if \"item\" in key]\n",
    "\n",
    "    def __compute_objectness(self, boxes):\n",
    "        target_matrix = np.zeros(49, dtype=np.float32).reshape(7, 7)\n",
    "        coords = []\n",
    "\n",
    "        for box in boxes:\n",
    "            box = box.tolist()\n",
    "            square_length = np.round(self.size/7, 1)\n",
    "            box_centerx, box_centery = np.round((box[2] - box[0]) / 2 + box[0], 1), np.round((box[3] - box[1]) / 2 + box[1], 1)\n",
    "            box_centerx, box_centery = math.floor(box_centerx / square_length), math.floor(box_centery / square_length)\n",
    "            target_matrix[box_centery, box_centerx] = 1.0\n",
    "            coords.append((box_centerx, box_centery))\n",
    "\n",
    "        return {\"matrix\": torch.as_tensor(target_matrix, dtype=torch.float32, device=custom_utils.DEVICE), \"coords\": coords}\n",
    "\n",
    "    def __generate_target(self, index):\n",
    "        \"\"\"\n",
    "        Generate the target dict according to Torch specification\n",
    "        :param index: i of the wanted annotations\n",
    "        :return: target dict\n",
    "        \"\"\"\n",
    "        annotations = self.__load_annotation(index)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        categories = []\n",
    "\n",
    "        for annotation in annotations:\n",
    "            boxes.append(annotation[\"bounding_box\"])\n",
    "            labels.append(annotation[\"category_id\"])\n",
    "            categories.append(annotation['category_name'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32, device=custom_utils.DEVICE)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64, device=custom_utils.DEVICE)\n",
    "\n",
    "        return {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"categories\": categories,\n",
    "            \"image_id\": torch.tensor([index], device=custom_utils.DEVICE)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading training dataset\n",
    "\n",
    "train_dataset = CustomDataset(os.path.join(custom_utils.PROJECT_ROOT, \"data\", \"assignment_1\", \"train\"))\n",
    "\n",
    "# plot_size_distribution(dataset)\n",
    "\n",
    "# random image\n",
    "# image, target = train_dataset[randint(0, len(train_dataset))]\n",
    "# transforms.ToPILImage()(target['objectness'][\"matrix\"]).show()\n",
    "\n",
    "# print(target['objectness'][\"matrix\"])\n",
    "\n",
    "# check bounding box\n",
    "\n",
    "# custom_utils.with_bounding_box(image, target).show()\n",
    "\n",
    "# Building training dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, collate_fn=custom_utils.collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.block1 = net_utils.build_low_level_feat(3, 16, 5, 4)\n",
    "        self.block2 = net_utils.build_low_level_feat(16, 32, 3, 2)\n",
    "        self.block3 = net_utils.build_low_level_feat(32, 64, 3, 2)\n",
    "        self.inception1 = net_utils.build_inception_components(64, 128)\n",
    "        self.inception2 = net_utils.build_inception_components(128*6, 128*12)\n",
    "        self.batch_after_inception2 = nn.BatchNorm2d(128*12*6)\n",
    "        self.activation_after_inception = nn.ReLU()\n",
    "        self.pool_after_inception = nn.MaxPool2d(2, 2)\n",
    "        self.output = net_utils.build_output_components(128*12*6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = [\n",
    "            self.inception1[0](x),\n",
    "            self.inception1[1](x),\n",
    "            self.inception1[2](x),\n",
    "            self.inception1[3](x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.activation_after_inception(x)\n",
    "        x = self.pool_after_inception(x)\n",
    "        x = [\n",
    "            self.inception2[0](x),\n",
    "            self.inception2[1](x),\n",
    "            self.inception2[2](x),\n",
    "            self.inception2[3](x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.batch_after_inception2(x)\n",
    "        x = self.activation_after_inception(x)\n",
    "        x = self.pool_after_inception(x)\n",
    "        x = [\n",
    "            self.output[0](x),\n",
    "            self.output[1](x),\n",
    "            self.output[2](x)\n",
    "        ]\n",
    "        return torch.cat(x, 1)\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, l1, l2, l3):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.l3 = l3\n",
    "\n",
    "    def forward(self, outputs, boxes, labels, objectness_list):\n",
    "        # Set up predicted values\n",
    "        p_boxes = []\n",
    "        p_labels = []\n",
    "        p_objectness = []\n",
    "        for img in outputs:\n",
    "            p_boxes.append(img[1:5])\n",
    "            p_labels.append(img[5:])\n",
    "            p_objectness.append(img[0].reshape(49))\n",
    "        p_boxes = torch.stack(p_boxes)\n",
    "        p_labels = torch.stack(p_labels)\n",
    "        p_objectness = torch.stack(p_objectness)\n",
    "\n",
    "        # Compute objectness loss\n",
    "        objectness = torch.stack([entry['matrix'] for entry in objectness_list]).reshape(BATCH, 49)\n",
    "        cel_obj_value = f.cross_entropy(p_objectness, objectness)\n",
    "\n",
    "        # Compute bb loss\n",
    "        objects_coords = [entry['coords'] for entry in objectness_list]\n",
    "        batch_bb_loss = 0\n",
    "        for i, objects in enumerate(objects_coords):\n",
    "            p_box = p_boxes[i]\n",
    "            for j, box in enumerate(objects):\n",
    "                p_box_coords = []\n",
    "                for filter in p_box:\n",
    "                    p_box_coords.append(filter[box[1]][box[0]].item())\n",
    "                p_box_coords = torch.tensor(p_box_coords, dtype=torch.float32, device=custom_utils.DEVICE)\n",
    "                batch_bb_loss += self.__compute_squared_error(p_box_coords, boxes[i][j])\n",
    "        batch_bb_loss /= BATCH\n",
    "\n",
    "        # Compute class loss\n",
    "        cel_class_value = 0\n",
    "        for i, objects in enumerate(objects_coords):\n",
    "            p_label = p_labels[i]\n",
    "            for j, box in enumerate(objects):\n",
    "                p_label_values = []\n",
    "                for filter in p_label:\n",
    "                    p_label_values.append(filter[box[1]][box[0]])\n",
    "                p_label_values = torch.tensor(p_label_values, dtype=torch.float32, device=custom_utils.DEVICE)\n",
    "                cel_class_value += f.cross_entropy(p_label_values, labels[i][j]-1)\n",
    "        cel_class_value /= BATCH\n",
    "\n",
    "        return cel_obj_value + batch_bb_loss + cel_class_value\n",
    "\n",
    "    def __compute_squared_error(self, x_pred, x_comp):\n",
    "        x_pred = x_pred.cpu()\n",
    "        x_comp = x_comp.cpu()\n",
    "\n",
    "        # v2 is scaled from image size to 1\n",
    "        scale = np.vectorize(lambda x: np.round(x / custom_utils.IMG_SIZE, 1))\n",
    "        x_comp_scaled = scale(x_comp)\n",
    "\n",
    "        components = [(x - x_cap)**2 for x, x_cap in zip(x_comp_scaled, x_pred)]\n",
    "\n",
    "        return sum(components)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = ObjectDetectionModel()\n",
    "loss_fn = YoloLoss(1, 5, 5)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "\n",
    "def train(num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    network.to(custom_utils.DEVICE)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            images, boxes, labels, objectness = data\n",
    "            images = images.to(custom_utils.DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(images)\n",
    "\n",
    "            loss = loss_fn(outputs, boxes, labels, objectness)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()  # extract the loss value\n",
    "            if i % 10 == 9:\n",
    "                # print every 1000 (twice per epoch)\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 10))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
