{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from rich.console import Console\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab block for loading data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__, '__IPYTHON__') else False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(\"drive/MyDrive/train_annotations.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/assignment_1/train/\")\n",
    "    with ZipFile(\"drive/MyDrive/test.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/assignment_1/\")\n",
    "    with ZipFile(\"drive/MyDrive/images_pt1.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/assignment_1/train/images/\")\n",
    "    with ZipFile(\"drive/MyDrive/images_pt2.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/assignment_1/train/images/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "# Set torch seed\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "# Initialize training variables\n",
    "BATCH = 16\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some custom utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUtils:\n",
    "  \n",
    "    # Defining project root in order to avoid relative paths\n",
    "    PROJECT_ROOT = \".\"\n",
    "\n",
    "    # Initializing torch device according to hardware available\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    IMG_SIZE = 256\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Function to combine images, boxes and labels\n",
    "        :param batch: an iterable of N sets from __getitem__() of CustomDataset\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes and labels\n",
    "        \"\"\"\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        mask_coords = list()\n",
    "        objectness_mask = list()\n",
    "        boxes_mask = list()\n",
    "        labels_mask = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1][\"boxes\"])\n",
    "            labels.append(b[1][\"labels\"])\n",
    "            mask_coords.append(b[1][\"objectness\"][\"coords\"])\n",
    "            objectness_mask.append(b[1][\"objectness\"][\"matrix\"])\n",
    "            boxes_mask.append(b[1][\"boxes_mask\"])\n",
    "            labels_mask.append(b[1][\"labels_mask\"])\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        objectness_mask = torch.stack(objectness_mask)\n",
    "        boxes_mask = torch.stack(boxes_mask)\n",
    "        labels_mask = torch.stack(labels_mask)\n",
    "\n",
    "        return images, (boxes, labels, mask_coords, objectness_mask, boxes_mask, labels_mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_bounding_box(image, target):\n",
    "        \"\"\"\n",
    "        Returns an image with bounding boxes and labels\n",
    "        :param image: image as Tensor\n",
    "        :param target: dict representing containing the bounding boxes\n",
    "        \"\"\"\n",
    "        tensor_image = torchvision.utils.draw_bounding_boxes(transforms.PILToTensor()(transforms.ToPILImage()(image)), target['boxes'], target['categories'], colors=\"red\", width=2)\n",
    "        return transforms.ToPILImage()(tensor_image)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_aspect_ratio_distribution(dataset):\n",
    "        \"\"\"\n",
    "        Returns the aspect ratio distribution of a CustomDataset\n",
    "        :param dataset: the dataset of type CustomDataset\n",
    "        \"\"\"\n",
    "        aspect_ratios = np.empty(len(dataset), dtype=float)\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            img, _ = dataset[i]\n",
    "            sizes = img.size\n",
    "            aspect_ratios = np.append(aspect_ratios, sizes[0] / sizes[1])\n",
    "\n",
    "        plt.bar(*np.unique(aspect_ratios, return_counts=True))\n",
    "        return plt\n",
    "\n",
    "    @staticmethod\n",
    "    def to_center_coords(boxes):\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            w = box[2] - box[0]\n",
    "            h = box[3] - box[1]\n",
    "            x = math.ceil(box[0] + w/2)\n",
    "            y = math.ceil(box[1] + h/2)\n",
    "            new_boxes.append([x, y, w, h])\n",
    "        return new_boxes\n",
    "\n",
    "    @staticmethod\n",
    "    def i_over_u(batched_predicted_boxes, batched_target_boxes):\n",
    "        \"\"\"\n",
    "        Compute intersection over union of batched Tensors\n",
    "        \"\"\"\n",
    "\n",
    "        pred_x1 = batched_predicted_boxes[..., 0:1] - batched_predicted_boxes[..., 2:3] / 2\n",
    "        pred_y1 = batched_predicted_boxes[..., 1:2] - batched_predicted_boxes[..., 3:4] / 2\n",
    "        pred_x2 = batched_predicted_boxes[..., 0:1] + batched_predicted_boxes[..., 2:3] / 2\n",
    "        pred_y2 = batched_predicted_boxes[..., 1:2] + batched_predicted_boxes[..., 3:4] / 2\n",
    "\n",
    "        target_x1 = batched_target_boxes[..., 0:1] - batched_target_boxes[..., 2:3] / 2\n",
    "        target_y1 = batched_target_boxes[..., 1:2] - batched_target_boxes[..., 3:4] / 2\n",
    "        target_x2 = batched_target_boxes[..., 0:1] + batched_target_boxes[..., 2:3] / 2\n",
    "        target_y2 = batched_target_boxes[..., 1:2] + batched_target_boxes[..., 3:4] / 2\n",
    "\n",
    "        intersection_area = (torch.min(pred_x2, target_x2) - torch.max(pred_x1, target_x1)).clamp(0) * (torch.min(pred_y2, target_y2) - torch.max(pred_y1, target_y1)).clamp(0)\n",
    "\n",
    "        pred_area = torch.abs((pred_x2 - pred_x1) * (pred_y2 - pred_y1))\n",
    "        target_area = torch.abs((target_x2 - target_x1) * (target_y2 - target_y1))\n",
    "\n",
    "        union_area = pred_area + target_area - intersection_area\n",
    "\n",
    "        return intersection_area/(union_area + 1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_low_level_feat(in_channels, out_channels, conv_k_size, pool_k_size):\n",
    "        \"\"\"\n",
    "        Builds a low level feature extraction block\n",
    "        :param in_channels: input channels for the block\n",
    "        :param out_channels: target output channels (there is no variation inside the block)\n",
    "        :param conv_k_size: kernel size for convolution\n",
    "        :param pool_k_size: kernel size for pooling | stride value\n",
    "        :return Sequential object [Conv -> ReLU -> Conv -> ReLU -> Conv -> BatchNorm -> ReLU -> MaxPool]\n",
    "        \"\"\"\n",
    "        layers = nn.Sequential()\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=conv_k_size, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=conv_k_size, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_k_size))\n",
    "        return layers\n",
    "\n",
    "    @staticmethod\n",
    "    def build_inception_components(in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Builds the inception network components\n",
    "        :param in_channels: input channels for the block\n",
    "        :param out_channels: for the four components will be [out_channels, out_channels, out_channels*2, out_channels*2]\n",
    "        :return the four components of an inception block\n",
    "        \"\"\"\n",
    "        pool = nn.Sequential(\n",
    "            nn.MaxPool2d(3, 1, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1).to(CustomUtils.DEVICE)\n",
    "        conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels * 2, kernel_size=1),\n",
    "            nn.Conv2d(out_channels * 2, out_channels * 2, kernel_size=3, padding=1)\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels * 2, kernel_size=1),\n",
    "            nn.Conv2d(out_channels * 2, out_channels * 2, kernel_size=5, padding=2)\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        return pool, conv1, conv2, conv3\n",
    "\n",
    "    @staticmethod\n",
    "    def build_output_components(in_channels, b=2):\n",
    "        \"\"\"\n",
    "        Builds the two output components of a YOLO-style network\n",
    "        :param in_channels: input channels for the block\n",
    "        :param b: number of boxes\n",
    "        :return the three components\n",
    "        \"\"\"\n",
    "        total_boxes_layers = b * 4\n",
    "        confidence = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, b, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        box = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, total_boxes_layers, 1),\n",
    "            nn.Conv2d(total_boxes_layers, total_boxes_layers, 9, padding='same'),\n",
    "            nn.Conv2d(total_boxes_layers, total_boxes_layers, 1),\n",
    "            nn.ReLU()\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        classes = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 13, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        ).to(CustomUtils.DEVICE)\n",
    "        \n",
    "        return confidence, box, classes\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a dataset object to use as input on a CNN\n",
    "    \"\"\"\n",
    "    def __init__(self, root):\n",
    "        \"\"\"\n",
    "        Default initializer\n",
    "        :param root: path to dataset root\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.size = CustomUtils.IMG_SIZE\n",
    "\n",
    "        # Load images filelist\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # Load annotations filelist\n",
    "        self.annotations = list(sorted(os.listdir(os.path.join(root, \"annotations\"))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Default getter for dataset objects\n",
    "        :param index: i of the wanted image + annotation\n",
    "        :return: image as PIL Image and target dictionary\n",
    "        \"\"\"\n",
    "        img = self.__load_image(index)\n",
    "        target = self.__generate_target(index)\n",
    "        if self.size is not None:\n",
    "            img, target = self.__apply_transform(img, target) \n",
    "\n",
    "        target[\"objectness\"] = self.__compute_objectness(target['boxes'])\n",
    "        target[\"boxes_mask\"] = self.__build_target_bb_mask(target['boxes'])\n",
    "        target[\"labels_mask\"] = self.__build_target_labels_mask(target['objectness']['coords'], target['labels'])\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __apply_transform(self, img, target):\n",
    "        \"\"\"\n",
    "        Apply a resize transformation to an image and its target\n",
    "        :param img: image as PIL Image\n",
    "        :param target: dict representing the bounding boxes\n",
    "        \"\"\"\n",
    "        target[\"boxes\"] = self.__resize_boxes(target[\"boxes\"], img.size)\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((self.size, self.size))])\n",
    "        img = transform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __resize_boxes(self, boxes, img_size):\n",
    "        \"\"\"\n",
    "        Apply to bounding boxes the same resize as the corresponding image\n",
    "        :param boxes: tensor containing the coordinates of the bounding boxes\n",
    "        :param img_size: size of the original image\n",
    "        \"\"\"\n",
    "        x_scale = self.size/img_size[0]\n",
    "        y_scale = self.size/img_size[1]\n",
    "\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            box = box.tolist()\n",
    "            x = int(np.round(box[0] * x_scale))\n",
    "            y = int(np.round(box[1] * y_scale))\n",
    "            x_max = int(np.round(box[2] * x_scale))\n",
    "            y_max = int(np.round(box[3] * y_scale))\n",
    "            scaled_boxes.append([x, y, x_max, y_max])\n",
    "        return torch.as_tensor(scaled_boxes, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
    "\n",
    "    def __load_image(self, index):\n",
    "        \"\"\"\n",
    "        Load an image from the list of available images\n",
    "        :param index: i of the wanted image\n",
    "        :return: the image as a PIL.Image object\n",
    "        \"\"\"\n",
    "        image_path = os.path.join(self.root, \"images\", self.images[index])\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    def __load_annotation(self, index):\n",
    "        \"\"\"\n",
    "        Load image annotations from the list of available annotations files\n",
    "        :param index: i of the wanted image\n",
    "        :return: the annotations as a dict\n",
    "        \"\"\"\n",
    "        annotation_path = os.path.join(self.root, \"annotations\", self.annotations[index])\n",
    "        with open(annotation_path, \"r\") as fp:\n",
    "            annotation_json = json.load(fp)\n",
    "        return [value for key, value in annotation_json.items() if \"item\" in key]\n",
    "\n",
    "    def __compute_objectness(self, boxes):\n",
    "        target_matrix = np.zeros(49, dtype=np.float32).reshape(7, 7)\n",
    "        coords = []\n",
    "        square_length = np.round(self.size/7, 1)\n",
    "\n",
    "        for box in boxes:\n",
    "            box = box.tolist()\n",
    "            box_center_x, box_center_y = np.round((box[2] - box[0]) / 2 + box[0], 1), np.round((box[3] - box[1]) / 2 + box[1], 1)\n",
    "            box_center_x, box_center_y = math.floor(box_center_x / square_length), math.floor(box_center_y / square_length)\n",
    "            target_matrix[box_center_y, box_center_x] = 1.0\n",
    "            coords.append((box_center_x, box_center_y))\n",
    "\n",
    "        return {\"matrix\": torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE), \"coords\": coords}\n",
    "\n",
    "    def __build_target_bb_mask(self, boxes):\n",
    "        target_matrix = np.zeros(49*4, dtype=np.float32).reshape((7, 7, 4))\n",
    "        square_length = np.round(self.size/7, 1)\n",
    "\n",
    "        for box in boxes:\n",
    "            box = box.tolist()\n",
    "            box_w = box[2] - box[0]\n",
    "            box_h = box[3] - box[1]\n",
    "            box_center_x = np.round(box[0] + box_w / 2, 1)\n",
    "            box_center_y = np.round(box[1] + box_h / 2, 1)\n",
    "            square_x, square_y = math.floor(box_center_x / square_length), math.floor(box_center_y / square_length)\n",
    "            square_corner_x, square_corner_y = square_x * square_length, square_y * square_length\n",
    "            box_center_x = (box_center_x - square_corner_x) / square_length\n",
    "            box_center_y = (box_center_y - square_corner_y) / square_length\n",
    "            box_w = box_w / self.size\n",
    "            box_h = box_h / self.size\n",
    "            target_matrix[square_y, square_x, 0] = box_center_x\n",
    "            target_matrix[square_y, square_x, 1] = box_center_y\n",
    "            target_matrix[square_y, square_x, 2] = box_w\n",
    "            target_matrix[square_y, square_x, 3] = box_h\n",
    "        return torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
    "\n",
    "    def __build_target_labels_mask(self, coords, labels):\n",
    "        target_matrix = np.zeros(49 * 13, dtype=np.float32).reshape((7, 7, 13))\n",
    "\n",
    "        labels = labels.tolist()\n",
    "        for index, label in enumerate(labels):\n",
    "            target_matrix[coords[index][1], coords[index][0], label-1] = 1.\n",
    "\n",
    "        return torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
    "\n",
    "    def __generate_target(self, index):\n",
    "        \"\"\"\n",
    "        Generate the target dict according to Torch specification\n",
    "        :param index: i of the wanted annotations\n",
    "        :return: target dict\n",
    "        \"\"\"\n",
    "        annotations = self.__load_annotation(index)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        categories = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            boxes.append(annotation[\"bounding_box\"])\n",
    "            labels.append(annotation[\"category_id\"])\n",
    "            categories.append(annotation['category_name'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64, device=CustomUtils.DEVICE)\n",
    "        \n",
    "        return {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"categories\": categories,\n",
    "            \"image_id\": torch.tensor([index], device=CustomUtils.DEVICE)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading training dataset \n",
    "\n",
    "train_dataset = CustomDataset(os.path.join(CustomUtils.PROJECT_ROOT, \"data\", \"assignment_1\", \"train\"))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, collate_fn=CustomUtils.collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_convolutions: int, out_filter: int, conv_k_sizes: list, pool_k_sizes: list):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        if len(conv_k_sizes) != len(pool_k_sizes) or len(conv_k_sizes) != num_convolutions or len(pool_k_sizes) != num_convolutions:\n",
    "            raise RuntimeError(\"Mismatch in length of arguments\")\n",
    "        in_filter = 3\n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        for i in range(num_convolutions):\n",
    "            block = CustomUtils.build_low_level_feat(in_filter, out_filter, conv_k_sizes[i], pool_k_sizes[i])\n",
    "            self.conv_blocks.append(block)\n",
    "            in_filter = out_filter\n",
    "            out_filter *= 2\n",
    "        self.inception1 = CustomUtils.build_inception_components(in_filter, out_filter)\n",
    "        # self.inception2 = net_utils.build_inception_components(128*6, 128*12)\n",
    "        self.batch_after_inception = nn.BatchNorm2d(out_filter*6)\n",
    "        self.activation_after_inception = nn.ReLU()\n",
    "        self.pool_after_inception = nn.MaxPool2d(2, 2)\n",
    "        self.output = CustomUtils.build_output_components(out_filter*6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = [\n",
    "            self.inception1[0](x),\n",
    "            self.inception1[1](x),\n",
    "            self.inception1[2](x),\n",
    "            self.inception1[3](x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.activation_after_inception(x)\n",
    "        x = self.pool_after_inception(x)\n",
    "        x = self.batch_after_inception(x)\n",
    "        x = [\n",
    "            self.output[0](x),\n",
    "            self.output[1](x),\n",
    "            self.output[2](x)\n",
    "        ]\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, l1, l2):\n",
    "        super(Loss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = predictions.reshape(-1, 7, 7, 23)\n",
    "        target_boxes_mask = targets[4]\n",
    "        objectness_mask = targets[3].unsqueeze(3)\n",
    "\n",
    "        iou_maxes, best_box = self.__find_best_bb(predictions[..., 2:10], target_boxes_mask)\n",
    "\n",
    "        box_loss = self.__compute_box_loss(predictions, target_boxes_mask, objectness_mask, best_box)\n",
    "\n",
    "        confidence_score = self.__compute_confidence_score(best_box, predictions)\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(objectness_mask * confidence_score),\n",
    "            torch.flatten(targets[3])\n",
    "        )\n",
    "\n",
    "        no_object_loss = self.__compute_no_object_loss(objectness_mask, predictions, targets[3])\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(objectness_mask * predictions[..., 10:], end_dim=-2),\n",
    "            torch.flatten(objectness_mask * targets[5], end_dim=-2)\n",
    "        )\n",
    "\n",
    "        return self.l1 * box_loss + object_loss + self.l2 * no_object_loss + class_loss, (self.l1 * box_loss, object_loss, self.l2 * no_object_loss, class_loss)\n",
    "\n",
    "    def __compute_box_loss(self, predictions, target_boxes_mask, objectness_mask, best_box):\n",
    "        box_predictions = self.__compute_valid_boxes(predictions[..., 2:10], objectness_mask, best_box)\n",
    "\n",
    "        box_targets = objectness_mask * target_boxes_mask\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        return self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2)\n",
    "        )\n",
    "\n",
    "    def __compute_no_object_loss(self, objectness_mask, predictions, targets):\n",
    "        no_obj_loss_1 = self.mse(\n",
    "            torch.flatten(((1 - objectness_mask) * predictions[..., 0:1]), start_dim=1),\n",
    "            torch.flatten(((1 - objectness_mask) * targets.unsqueeze(3)), start_dim=1)\n",
    "        )\n",
    "        no_obj_loss_2 = self.mse(\n",
    "            torch.flatten(((1 - objectness_mask) * predictions[..., 1:2]), start_dim=1),\n",
    "            torch.flatten(((1 - objectness_mask) * targets.unsqueeze(3)), start_dim=1)\n",
    "        )\n",
    "        return no_obj_loss_1 + no_obj_loss_2\n",
    "\n",
    "    def __compute_confidence_score(self, best_box, predictions):\n",
    "        \"\"\"\n",
    "            Compute confidence score according to YOLOv1\n",
    "        \"\"\"\n",
    "        return best_box * predictions[..., 1:2] + (1 - best_box) * predictions[..., 0:1]\n",
    "\n",
    "    def __compute_valid_boxes(self, predictions, objectness_mask, best_box):\n",
    "        \"\"\"\n",
    "        Computes the valid predictions based on best bounding box and valid objectness\n",
    "        \"\"\"\n",
    "        return objectness_mask * (best_box * predictions[..., 4:8] + (1 - best_box) * predictions[..., 0:4])\n",
    "\n",
    "    def __find_best_bb(self, predictions, target_boxes_mask):\n",
    "        \"\"\"\n",
    "        Computes the best predicted bounding box using Intersection Over Union\n",
    "        \"\"\"\n",
    "        iou_bb_1 = CustomUtils.i_over_u(predictions[..., 0:4], target_boxes_mask)  # Computes IOU on first predicted bounding box and target\n",
    "        iou_bb_2 = CustomUtils.i_over_u(predictions[..., 4:8], target_boxes_mask)  # Computes IOU on first predicted bounding box and target\n",
    "        iou_bbs = torch.cat([iou_bb_1.unsqueeze(0), iou_bb_2.unsqueeze(0)], dim=0)  # Merge the previous two into a (2, BATCH, 7, 7, 4) Tensor\n",
    "        return torch.max(iou_bbs, dim=0)  # Return best bounding box for each mask cell (maximum IOU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_convolutions = 3\n",
    "out_filter = 16\n",
    "conv_k_sizes = [5, 5, 3]\n",
    "pool_k_sizes = [4, 2, 2]\n",
    "network = ObjectDetectionModel(num_convolutions, out_filter, conv_k_sizes, pool_k_sizes)\n",
    "loss_fn = Loss(5, 0.5)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    network.to(CustomUtils.DEVICE)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(tqdm(train_dataloader)):\n",
    "            images, target = data\n",
    "            images = images.to(CustomUtils.DEVICE)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(images)\n",
    "\n",
    "            loss_fn_return = loss_fn(outputs, target)\n",
    "            loss = loss_fn_return[0]\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            bb = loss_fn_return[1][0]\n",
    "            obj = loss_fn_return[1][1]\n",
    "            no_obj = loss_fn_return[1][2]\n",
    "            cla = loss_fn_return[1][3]\n",
    "            if i % 10 == 9:\n",
    "                print(\n",
    "                    '[%d, %5d] loss: %.3f - bb: %.3f | obj: %.3f | no_obj: %.3f | class: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 10, bb, obj, no_obj, cla)\n",
    "                )\n",
    "                running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1141 [00:03<31:50,  1.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\u001b[39m3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [40], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     24\u001b[0m bb \u001b[39m=\u001b[39m loss_fn_return[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m obj \u001b[39m=\u001b[39m loss_fn_return[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "53ca6db60e8572b5cca1a2505e0e8f40644b2c5d709dfa1cf5346fd4aed1282b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
