{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMsnFc4DJk4v"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YBydRq5VJk4x",
        "outputId": "b946380f-b362-46ec-da2f-c338ff3e53a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.8/dist-packages (12.6.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rich\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from rich.console import Console\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUM0cusEJk4z"
      },
      "source": [
        "## Colab block for loading data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QsVKyjIAJk41",
        "outputId": "861be998-55d1-4e79-90ca-a19cc5fa9c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "IN_COLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__, '__IPYTHON__') else False\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from zipfile import ZipFile\n",
        "    with ZipFile(\"drive/MyDrive/train_annotations.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/assignment_1/train/\")\n",
        "    with ZipFile(\"drive/MyDrive/test.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/assignment_1/\")\n",
        "    with ZipFile(\"drive/MyDrive/images_pt1.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/assignment_1/train/images/\")\n",
        "    with ZipFile(\"drive/MyDrive/images_pt2.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/assignment_1/train/images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXgFChsJk43"
      },
      "source": [
        "## Initial configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TTRKfYOKJk43"
      },
      "outputs": [],
      "source": [
        "console = Console()\n",
        "\n",
        "# Set torch seed\n",
        "torch.manual_seed(3407)\n",
        "\n",
        "# Initialize training variables\n",
        "BATCH = 4\n",
        "LR = 0.001\n",
        "MOMENTUM = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQMgCqbJk44"
      },
      "source": [
        "## Defining some custom utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jj370Xz6Jk44"
      },
      "outputs": [],
      "source": [
        "class CustomUtils:\n",
        "  \n",
        "    # Defining project root in order to avoid relative paths\n",
        "    PROJECT_ROOT = \".\"\n",
        "\n",
        "    # Initializing torch device according to hardware available\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    IMG_SIZE = 256\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        \"\"\"\n",
        "        Function to combine images, boxes and labels\n",
        "        :param batch: an iterable of N sets from __getitem__() of CustomDataset\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes and labels\n",
        "        \"\"\"\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "        mask_coords = list()\n",
        "        objectness_mask = list()\n",
        "        boxes_mask = list()\n",
        "        labels_mask = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            boxes.append(b[1][\"boxes\"])\n",
        "            labels.append(b[1][\"labels\"])\n",
        "            mask_coords.append(b[1][\"objectness\"][\"coords\"])\n",
        "            objectness_mask.append(b[1][\"objectness\"][\"matrix\"])\n",
        "            boxes_mask.append(b[1][\"boxes_mask\"])\n",
        "            labels_mask.append(b[1][\"labels_mask\"])\n",
        "\n",
        "        images = torch.stack(images)\n",
        "        objectness_mask = torch.stack(objectness_mask)\n",
        "        boxes_mask = torch.stack(boxes_mask)\n",
        "        labels_mask = torch.stack(labels_mask)\n",
        "\n",
        "        return images, (boxes, labels, mask_coords, objectness_mask, boxes_mask, labels_mask)\n",
        "\n",
        "    @staticmethod\n",
        "    def with_bounding_box(image, target):\n",
        "        \"\"\"\n",
        "        Returns an image with bounding boxes and labels\n",
        "        :param image: image as Tensor\n",
        "        :param target: dict representing containing the bounding boxes\n",
        "        \"\"\"\n",
        "        tensor_image = torchvision.utils.draw_bounding_boxes(transforms.PILToTensor()(transforms.ToPILImage()(image)), target['boxes'], target['categories'], colors=\"red\", width=2)\n",
        "        return transforms.ToPILImage()(tensor_image)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_aspect_ratio_distribution(dataset):\n",
        "        \"\"\"\n",
        "        Returns the aspect ratio distribution of a CustomDataset\n",
        "        :param dataset: the dataset of type CustomDataset\n",
        "        \"\"\"\n",
        "        aspect_ratios = np.empty(len(dataset), dtype=float)\n",
        "        for i in tqdm(range(len(dataset))):\n",
        "            img, _ = dataset[i]\n",
        "            sizes = img.size\n",
        "            aspect_ratios = np.append(aspect_ratios, sizes[0] / sizes[1])\n",
        "\n",
        "        plt.bar(*np.unique(aspect_ratios, return_counts=True))\n",
        "        return plt\n",
        "\n",
        "    @staticmethod\n",
        "    def to_center_coords(boxes):\n",
        "        new_boxes = []\n",
        "        for box in boxes:\n",
        "            w = box[2] - box[0]\n",
        "            h = box[3] - box[1]\n",
        "            x = math.ceil(box[0] + w/2)\n",
        "            y = math.ceil(box[1] + h/2)\n",
        "            new_boxes.append([x, y, w, h])\n",
        "        return new_boxes\n",
        "\n",
        "    @staticmethod\n",
        "    def i_over_u(batched_predicted_boxes, batched_target_boxes):\n",
        "        \"\"\"\n",
        "        Compute intersection over union of batched Tensors\n",
        "        \"\"\"\n",
        "\n",
        "        pred_x1 = batched_predicted_boxes[..., 0:1] - batched_predicted_boxes[..., 2:3] / 2\n",
        "        pred_y1 = batched_predicted_boxes[..., 1:2] - batched_predicted_boxes[..., 3:4] / 2\n",
        "        pred_x2 = batched_predicted_boxes[..., 0:1] + batched_predicted_boxes[..., 2:3] / 2\n",
        "        pred_y2 = batched_predicted_boxes[..., 1:2] + batched_predicted_boxes[..., 3:4] / 2\n",
        "\n",
        "        target_x1 = batched_target_boxes[..., 0:1] - batched_target_boxes[..., 2:3] / 2\n",
        "        target_y1 = batched_target_boxes[..., 1:2] - batched_target_boxes[..., 3:4] / 2\n",
        "        target_x2 = batched_target_boxes[..., 0:1] + batched_target_boxes[..., 2:3] / 2\n",
        "        target_y2 = batched_target_boxes[..., 1:2] + batched_target_boxes[..., 3:4] / 2\n",
        "\n",
        "        intersection_area = (torch.min(pred_x2, target_x2) - torch.max(pred_x1, target_x1)).clamp(0) * (torch.min(pred_y2, target_y2) - torch.max(pred_y1, target_y1)).clamp(0)\n",
        "\n",
        "        pred_area = torch.abs((pred_x2 - pred_x1) * (pred_y2 - pred_y1))\n",
        "        target_area = torch.abs((target_x2 - target_x1) * (target_y2 - target_y1))\n",
        "\n",
        "        union_area = pred_area + target_area - intersection_area\n",
        "\n",
        "        return intersection_area/(union_area + 1e-8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def build_low_level_feat(in_channels, out_channels, conv_k_size, pool_k_size):\n",
        "        \"\"\"\n",
        "        Builds a low level feature extraction block\n",
        "        :param in_channels: input channels for the block\n",
        "        :param out_channels: target output channels (there is no variation inside the block)\n",
        "        :param conv_k_size: kernel size for convolution\n",
        "        :param pool_k_size: kernel size for pooling | stride value\n",
        "        :return Sequential object [Conv -> ReLU -> Conv -> ReLU -> Conv -> BatchNorm -> ReLU -> MaxPool]\n",
        "        \"\"\"\n",
        "        layers = nn.Sequential()\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=conv_k_size, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=conv_k_size, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_k_size))\n",
        "        return layers\n",
        "\n",
        "    @staticmethod\n",
        "    def build_inception_components(in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Builds the inception network components\n",
        "        :param in_channels: input channels for the block\n",
        "        :param out_channels: for the four components will be [out_channels, out_channels, out_channels*2, out_channels*2]\n",
        "        :return the four components of an inception block\n",
        "        \"\"\"\n",
        "        pool = nn.Sequential(\n",
        "            nn.MaxPool2d(3, 1, padding=1),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1).to(CustomUtils.DEVICE)\n",
        "        conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels * 2, kernel_size=1),\n",
        "            nn.Conv2d(out_channels * 2, out_channels * 2, kernel_size=3, padding=1)\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels * 2, kernel_size=1),\n",
        "            nn.Conv2d(out_channels * 2, out_channels * 2, kernel_size=5, padding=2)\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        return pool, conv1, conv2, conv3\n",
        "\n",
        "    @staticmethod\n",
        "    def build_output_components(in_channels, b=2):\n",
        "        \"\"\"\n",
        "        Builds the two output components of a YOLO-style network\n",
        "        :param in_channels: input channels for the block\n",
        "        :param b: number of boxes\n",
        "        :return the three components\n",
        "        \"\"\"\n",
        "        total_boxes_layers = b * 4\n",
        "        confidence = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, b, 1),\n",
        "            nn.Sigmoid()\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        box = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, total_boxes_layers, 1),\n",
        "            nn.Conv2d(total_boxes_layers, total_boxes_layers, 9, padding='same'),\n",
        "            nn.Conv2d(total_boxes_layers, total_boxes_layers, 1),\n",
        "            nn.ReLU()\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        classes = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 13, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        ).to(CustomUtils.DEVICE)\n",
        "        \n",
        "        return confidence, box, classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaZ7UfpjJk46"
      },
      "source": [
        "## Defining Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DwhqCT0-Jk47"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a dataset object to use as input on a CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, root):\n",
        "        \"\"\"\n",
        "        Default initializer\n",
        "        :param root: path to dataset root\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.size = CustomUtils.IMG_SIZE\n",
        "\n",
        "        # Load images filelist\n",
        "        self.images = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        # Load annotations filelist\n",
        "        self.annotations = list(sorted(os.listdir(os.path.join(root, \"annotations\"))))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Default getter for dataset objects\n",
        "        :param index: i of the wanted image + annotation\n",
        "        :return: image as PIL Image and target dictionary\n",
        "        \"\"\"\n",
        "        img = self.__load_image(index)\n",
        "        target = self.__generate_target(index)\n",
        "        if self.size is not None:\n",
        "            img, target = self.__apply_transform(img, target) \n",
        "\n",
        "        target[\"objectness\"] = self.__compute_objectness(target['boxes'])\n",
        "        target[\"boxes_mask\"] = self.__build_target_bb_mask(target['boxes'])\n",
        "        target[\"labels_mask\"] = self.__build_target_labels_mask(target['objectness']['coords'], target['labels'])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __apply_transform(self, img, target):\n",
        "        \"\"\"\n",
        "        Apply a resize transformation to an image and its target\n",
        "        :param img: image as PIL Image\n",
        "        :param target: dict representing the bounding boxes\n",
        "        \"\"\"\n",
        "        target[\"boxes\"] = self.__resize_boxes(target[\"boxes\"], img.size)\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((self.size, self.size))])\n",
        "        img = transform(img)\n",
        "        return img, target\n",
        "\n",
        "    def __resize_boxes(self, boxes, img_size):\n",
        "        \"\"\"\n",
        "        Apply to bounding boxes the same resize as the corresponding image\n",
        "        :param boxes: tensor containing the coordinates of the bounding boxes\n",
        "        :param img_size: size of the original image\n",
        "        \"\"\"\n",
        "        x_scale = self.size/img_size[0]\n",
        "        y_scale = self.size/img_size[1]\n",
        "\n",
        "        scaled_boxes = []\n",
        "        for box in boxes:\n",
        "            box = box.tolist()\n",
        "            x = int(np.round(box[0] * x_scale))\n",
        "            y = int(np.round(box[1] * y_scale))\n",
        "            x_max = int(np.round(box[2] * x_scale))\n",
        "            y_max = int(np.round(box[3] * y_scale))\n",
        "            scaled_boxes.append([x, y, x_max, y_max])\n",
        "        return torch.as_tensor(scaled_boxes, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
        "\n",
        "    def __load_image(self, index):\n",
        "        \"\"\"\n",
        "        Load an image from the list of available images\n",
        "        :param index: i of the wanted image\n",
        "        :return: the image as a PIL.Image object\n",
        "        \"\"\"\n",
        "        image_path = os.path.join(self.root, \"images\", self.images[index])\n",
        "        return Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    def __load_annotation(self, index):\n",
        "        \"\"\"\n",
        "        Load image annotations from the list of available annotations files\n",
        "        :param index: i of the wanted image\n",
        "        :return: the annotations as a dict\n",
        "        \"\"\"\n",
        "        annotation_path = os.path.join(self.root, \"annotations\", self.annotations[index])\n",
        "        with open(annotation_path, \"r\") as fp:\n",
        "            annotation_json = json.load(fp)\n",
        "        return [value for key, value in annotation_json.items() if \"item\" in key]\n",
        "\n",
        "    def __compute_objectness(self, boxes):\n",
        "        target_matrix = np.zeros(49, dtype=np.float32).reshape(7, 7)\n",
        "        coords = []\n",
        "        square_length = np.round(self.size/7, 1)\n",
        "\n",
        "        for box in boxes:\n",
        "            box = box.tolist()\n",
        "            box_center_x, box_center_y = np.round((box[2] - box[0]) / 2 + box[0], 1), np.round((box[3] - box[1]) / 2 + box[1], 1)\n",
        "            box_center_x, box_center_y = math.floor(box_center_x / square_length), math.floor(box_center_y / square_length)\n",
        "            target_matrix[box_center_y, box_center_x] = 1.0\n",
        "            coords.append((box_center_x, box_center_y))\n",
        "\n",
        "        return {\"matrix\": torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE), \"coords\": coords}\n",
        "\n",
        "    def __build_target_bb_mask(self, boxes):\n",
        "        target_matrix = np.zeros(49*4, dtype=np.float32).reshape((7, 7, 4))\n",
        "        square_length = np.round(self.size/7, 1)\n",
        "\n",
        "        for box in boxes:\n",
        "            box = box.tolist()\n",
        "            box_w = box[2] - box[0]\n",
        "            box_h = box[3] - box[1]\n",
        "            box_center_x = np.round(box[0] + box_w / 2, 1)\n",
        "            box_center_y = np.round(box[1] + box_h / 2, 1)\n",
        "            square_x, square_y = math.floor(box_center_x / square_length), math.floor(box_center_y / square_length)\n",
        "            square_corner_x, square_corner_y = square_x * square_length, square_y * square_length\n",
        "            box_center_x = (box_center_x - square_corner_x) / square_length\n",
        "            box_center_y = (box_center_y - square_corner_y) / square_length\n",
        "            box_w = box_w / self.size\n",
        "            box_h = box_h / self.size\n",
        "            target_matrix[square_y, square_x, 0] = box_center_x\n",
        "            target_matrix[square_y, square_x, 1] = box_center_y\n",
        "            target_matrix[square_y, square_x, 2] = box_w\n",
        "            target_matrix[square_y, square_x, 3] = box_h\n",
        "        return torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
        "\n",
        "    def __build_target_labels_mask(self, coords, labels):\n",
        "        target_matrix = np.zeros(49 * 13, dtype=np.float32).reshape((7, 7, 13))\n",
        "\n",
        "        labels = labels.tolist()\n",
        "        for index, label in enumerate(labels):\n",
        "            target_matrix[coords[index][1], coords[index][0], label-1] = 1.\n",
        "\n",
        "        return torch.as_tensor(target_matrix, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
        "\n",
        "    def __generate_target(self, index):\n",
        "        \"\"\"\n",
        "        Generate the target dict according to Torch specification\n",
        "        :param index: i of the wanted annotations\n",
        "        :return: target dict\n",
        "        \"\"\"\n",
        "        annotations = self.__load_annotation(index)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        categories = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            boxes.append(annotation[\"bounding_box\"])\n",
        "            labels.append(annotation[\"category_id\"])\n",
        "            categories.append(annotation['category_name'])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32, device=CustomUtils.DEVICE)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64, device=CustomUtils.DEVICE)\n",
        "        \n",
        "        return {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"categories\": categories,\n",
        "            \"image_id\": torch.tensor([index], device=CustomUtils.DEVICE)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ZNuidPJk49"
      },
      "source": [
        "## Training dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4diGxwSDJk4-"
      },
      "outputs": [],
      "source": [
        "# Loading training dataset \n",
        "\n",
        "train_dataset = CustomDataset(os.path.join(CustomUtils.PROJECT_ROOT, \"data\", \"assignment_1\", \"train\"))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, collate_fn=CustomUtils.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2INRhvXyJk4_"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wKkQC3WkJk4_"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionModel(nn.Module):\n",
        "    def __init__(self, num_convolutions: int, out_filter: int, conv_k_sizes: list, pool_k_sizes: list):\n",
        "        super(ObjectDetectionModel, self).__init__()\n",
        "        if len(conv_k_sizes) != len(pool_k_sizes) or len(conv_k_sizes) != num_convolutions or len(pool_k_sizes) != num_convolutions:\n",
        "            raise RuntimeError(\"Mismatch in length of arguments\")\n",
        "        in_filter = 3\n",
        "        self.conv_blocks = nn.Sequential()\n",
        "        for i in range(num_convolutions):\n",
        "            block = CustomUtils.build_low_level_feat(in_filter, out_filter, conv_k_sizes[i], pool_k_sizes[i])\n",
        "            self.conv_blocks.append(block)\n",
        "            in_filter = out_filter\n",
        "            out_filter *= 2\n",
        "        self.inception1 = CustomUtils.build_inception_components(in_filter, out_filter)\n",
        "        in_filter = out_filter * 6\n",
        "        out_filter = in_filter * 2\n",
        "        self.inception2 = CustomUtils.build_inception_components(in_filter, out_filter)\n",
        "        self.batch_after_inception2 = nn.BatchNorm2d(out_filter*6)\n",
        "        self.activation_after_inception = nn.ReLU()\n",
        "        self.pool_after_inception = nn.MaxPool2d(2, 2)\n",
        "        self.output = CustomUtils.build_output_components(out_filter*6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_blocks(x)\n",
        "        x = [\n",
        "            self.inception1[0](x),\n",
        "            self.inception1[1](x),\n",
        "            self.inception1[2](x),\n",
        "            self.inception1[3](x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.activation_after_inception(x)\n",
        "        x = self.pool_after_inception(x)\n",
        "        x = [\n",
        "            self.inception2[0](x),\n",
        "            self.inception2[1](x),\n",
        "            self.inception2[2](x),\n",
        "            self.inception2[3](x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.activation_after_inception(x)\n",
        "        x = self.pool_after_inception(x)\n",
        "        x = self.batch_after_inception2(x)\n",
        "        x = [\n",
        "            self.output[0](x),\n",
        "            self.output[1](x),\n",
        "            self.output[2](x)\n",
        "        ]\n",
        "        return torch.cat(x, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBw2y_PeJk5A"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-HI--JUjJk5B"
      },
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, l1, l2):\n",
        "        super(Loss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.l1 = l1\n",
        "        self.l2 = l2\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        predictions = predictions.reshape(-1, 7, 7, 23)\n",
        "        target_boxes_mask = targets[4]\n",
        "        objectness_mask = targets[3].unsqueeze(3)\n",
        "\n",
        "        iou_maxes, best_box = self.__find_best_bb(predictions[..., 2:10], target_boxes_mask)\n",
        "\n",
        "        box_loss = self.__compute_box_loss(predictions, target_boxes_mask, objectness_mask, best_box)\n",
        "\n",
        "        confidence_score = self.__compute_confidence_score(best_box, predictions)\n",
        "\n",
        "        object_loss = self.mse(\n",
        "            torch.flatten(objectness_mask * confidence_score),\n",
        "            torch.flatten(targets[3])\n",
        "        )\n",
        "\n",
        "        no_object_loss = self.__compute_no_object_loss(objectness_mask, predictions, targets[3])\n",
        "\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(objectness_mask * predictions[..., 10:], end_dim=-2),\n",
        "            torch.flatten(objectness_mask * targets[5], end_dim=-2)\n",
        "        )\n",
        "\n",
        "        return self.l1 * box_loss + object_loss + self.l2 * no_object_loss + class_loss, (self.l1 * box_loss, object_loss, self.l2 * no_object_loss, class_loss)\n",
        "\n",
        "    def __compute_box_loss(self, predictions, target_boxes_mask, objectness_mask, best_box):\n",
        "        box_predictions = self.__compute_valid_boxes(predictions[..., 2:10], objectness_mask, best_box)\n",
        "\n",
        "        box_targets = objectness_mask * target_boxes_mask\n",
        "\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        return self.mse(\n",
        "            torch.flatten(box_predictions, end_dim=-2),\n",
        "            torch.flatten(box_targets, end_dim=-2)\n",
        "        )\n",
        "\n",
        "    def __compute_no_object_loss(self, objectness_mask, predictions, targets):\n",
        "        no_obj_loss_1 = self.mse(\n",
        "            torch.flatten(((1 - objectness_mask) * predictions[..., 0:1]), start_dim=1),\n",
        "            torch.flatten(((1 - objectness_mask) * targets.unsqueeze(3)), start_dim=1)\n",
        "        )\n",
        "        no_obj_loss_2 = self.mse(\n",
        "            torch.flatten(((1 - objectness_mask) * predictions[..., 1:2]), start_dim=1),\n",
        "            torch.flatten(((1 - objectness_mask) * targets.unsqueeze(3)), start_dim=1)\n",
        "        )\n",
        "        return no_obj_loss_1 + no_obj_loss_2\n",
        "\n",
        "    def __compute_confidence_score(self, best_box, predictions):\n",
        "        \"\"\"\n",
        "            Compute confidence score according to YOLOv1\n",
        "        \"\"\"\n",
        "        return best_box * predictions[..., 1:2] + (1 - best_box) * predictions[..., 0:1]\n",
        "\n",
        "    def __compute_valid_boxes(self, predictions, objectness_mask, best_box):\n",
        "        \"\"\"\n",
        "        Computes the valid predictions based on best bounding box and valid objectness\n",
        "        \"\"\"\n",
        "        return objectness_mask * (best_box * predictions[..., 4:8] + (1 - best_box) * predictions[..., 0:4])\n",
        "\n",
        "    def __find_best_bb(self, predictions, target_boxes_mask):\n",
        "        \"\"\"\n",
        "        Computes the best predicted bounding box using Intersection Over Union\n",
        "        \"\"\"\n",
        "        iou_bb_1 = CustomUtils.i_over_u(predictions[..., 0:4], target_boxes_mask)  # Computes IOU on first predicted bounding box and target\n",
        "        iou_bb_2 = CustomUtils.i_over_u(predictions[..., 4:8], target_boxes_mask)  # Computes IOU on first predicted bounding box and target\n",
        "        iou_bbs = torch.cat([iou_bb_1.unsqueeze(0), iou_bb_2.unsqueeze(0)], dim=0)  # Merge the previous two into a (2, BATCH, 7, 7, 4) Tensor\n",
        "        return torch.max(iou_bbs, dim=0)  # Return best bounding box for each mask cell (maximum IOU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n6W3rHAhJk5C"
      },
      "outputs": [],
      "source": [
        "num_convolutions = 4\n",
        "out_filter = 16\n",
        "conv_k_sizes = [5, 3, 3, 3]\n",
        "pool_k_sizes = [2, 2, 2, 1]\n",
        "network = ObjectDetectionModel(num_convolutions, out_filter, conv_k_sizes, pool_k_sizes)\n",
        "loss_fn = Loss(5, 0.5)\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=LR, max_lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nu_W7sTSJk5C"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs):\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    network.to(CustomUtils.DEVICE)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.\n",
        "\n",
        "        for i, data in enumerate(tqdm(train_dataloader)):\n",
        "            images, target = data\n",
        "            images = images.to(CustomUtils.DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = network(images)\n",
        "\n",
        "            loss_fn_return = loss_fn(outputs, target)\n",
        "            loss = loss_fn_return[0]\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            bb = loss_fn_return[1][0]\n",
        "            obj = loss_fn_return[1][1]\n",
        "            no_obj = loss_fn_return[1][2]\n",
        "            cla = loss_fn_return[1][3]\n",
        "            if i % 10 == 9:\n",
        "                print(\n",
        "                    '[%d, %5d] loss: %.3f - bb: %.3f | obj: %.3f | no_obj: %.3f | class: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 10, bb, obj, no_obj, cla)\n",
        "                )\n",
        "                running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwvuymPJJk5D",
        "outputId": "242bad4e-a90c-44a2-9bdd-fa8482da3d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 10/4564 [00:08<24:52,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 56.168 - bb: 38.131 | obj: 5.860 | no_obj: 8.185 | class: 6.992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 20/4564 [00:11<18:09,  4.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    20] loss: 48.975 - bb: 38.789 | obj: 6.563 | no_obj: 7.704 | class: 7.737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 30/4564 [00:13<18:05,  4.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    30] loss: 45.568 - bb: 17.053 | obj: 4.729 | no_obj: 6.758 | class: 5.924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 40/4564 [00:16<18:36,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    40] loss: 40.782 - bb: 35.767 | obj: 7.428 | no_obj: 5.547 | class: 9.440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 50/4564 [00:18<17:59,  4.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    50] loss: 50.105 - bb: 27.554 | obj: 5.017 | no_obj: 4.545 | class: 6.321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 60/4564 [00:21<18:28,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    60] loss: 46.177 - bb: 14.875 | obj: 3.470 | no_obj: 4.282 | class: 5.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 70/4564 [00:23<17:14,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    70] loss: 31.507 - bb: 11.731 | obj: 4.000 | no_obj: 3.298 | class: 6.495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 80/4564 [00:25<17:55,  4.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    80] loss: 42.227 - bb: 9.722 | obj: 3.987 | no_obj: 3.146 | class: 7.524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 90/4564 [00:28<18:07,  4.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    90] loss: 36.833 - bb: 12.043 | obj: 2.904 | no_obj: 2.910 | class: 7.655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 100/4564 [00:30<18:06,  4.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 41.170 - bb: 36.112 | obj: 6.895 | no_obj: 3.017 | class: 9.651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 110/4564 [00:33<17:17,  4.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   110] loss: 39.182 - bb: 24.973 | obj: 4.261 | no_obj: 3.134 | class: 7.455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 120/4564 [00:35<18:27,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   120] loss: 40.187 - bb: 41.608 | obj: 6.547 | no_obj: 3.783 | class: 10.028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 130/4564 [00:38<18:05,  4.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   130] loss: 36.093 - bb: 27.814 | obj: 4.528 | no_obj: 2.676 | class: 8.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 140/4564 [00:40<18:23,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   140] loss: 39.498 - bb: 14.499 | obj: 5.389 | no_obj: 2.311 | class: 9.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 150/4564 [00:43<17:20,  4.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   150] loss: 34.328 - bb: 11.786 | obj: 1.878 | no_obj: 2.686 | class: 5.334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 160/4564 [00:45<17:30,  4.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   160] loss: 29.343 - bb: 20.152 | obj: 4.045 | no_obj: 2.641 | class: 7.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 170/4564 [00:47<17:04,  4.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   170] loss: 34.877 - bb: 19.960 | obj: 5.046 | no_obj: 2.368 | class: 10.570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 180/4564 [00:50<17:13,  4.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   180] loss: 29.858 - bb: 26.575 | obj: 4.849 | no_obj: 2.687 | class: 6.533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 190/4564 [00:52<17:52,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   190] loss: 37.287 - bb: 15.274 | obj: 3.719 | no_obj: 2.318 | class: 7.464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 200/4564 [00:55<16:46,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 35.676 - bb: 22.322 | obj: 2.287 | no_obj: 2.461 | class: 5.331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 210/4564 [00:57<17:35,  4.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   210] loss: 33.059 - bb: 10.883 | obj: 2.977 | no_obj: 2.147 | class: 7.375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 220/4564 [00:59<17:19,  4.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   220] loss: 30.132 - bb: 7.190 | obj: 2.269 | no_obj: 1.995 | class: 7.225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 230/4564 [01:02<17:14,  4.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   230] loss: 28.505 - bb: 12.054 | obj: 2.966 | no_obj: 2.467 | class: 9.038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 240/4564 [01:04<16:54,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   240] loss: 27.098 - bb: 4.579 | obj: 2.582 | no_obj: 2.430 | class: 7.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 250/4564 [01:06<17:33,  4.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   250] loss: 28.717 - bb: 25.829 | obj: 6.287 | no_obj: 2.817 | class: 10.116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 260/4564 [01:09<18:03,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   260] loss: 32.965 - bb: 23.739 | obj: 5.382 | no_obj: 2.486 | class: 9.671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 270/4564 [01:11<17:15,  4.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   270] loss: 29.699 - bb: 15.780 | obj: 4.560 | no_obj: 2.305 | class: 8.230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 280/4564 [01:14<16:47,  4.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   280] loss: 28.250 - bb: 11.688 | obj: 3.775 | no_obj: 2.249 | class: 6.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 290/4564 [01:16<17:56,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   290] loss: 30.756 - bb: 16.451 | obj: 5.139 | no_obj: 2.094 | class: 9.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 300/4564 [01:19<16:41,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   300] loss: 31.149 - bb: 18.652 | obj: 4.708 | no_obj: 2.328 | class: 7.711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 310/4564 [01:21<16:50,  4.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   310] loss: 28.974 - bb: 13.672 | obj: 3.831 | no_obj: 1.743 | class: 8.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 320/4564 [01:23<16:53,  4.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   320] loss: 26.403 - bb: 15.442 | obj: 4.770 | no_obj: 2.059 | class: 9.560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 330/4564 [01:26<16:58,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   330] loss: 27.312 - bb: 9.727 | obj: 4.004 | no_obj: 1.974 | class: 7.497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 340/4564 [01:28<16:48,  4.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   340] loss: 26.617 - bb: 13.725 | obj: 3.725 | no_obj: 2.134 | class: 6.512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 350/4564 [01:31<16:50,  4.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   350] loss: 24.985 - bb: 17.994 | obj: 4.018 | no_obj: 2.428 | class: 6.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 360/4564 [01:33<16:20,  4.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   360] loss: 30.883 - bb: 8.876 | obj: 3.999 | no_obj: 2.181 | class: 8.107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 370/4564 [01:35<16:33,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   370] loss: 27.514 - bb: 11.140 | obj: 4.719 | no_obj: 1.955 | class: 7.977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 380/4564 [01:38<17:06,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   380] loss: 23.074 - bb: 9.295 | obj: 3.150 | no_obj: 1.917 | class: 8.526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 386/4564 [01:39<17:31,  3.97it/s]"
          ]
        }
      ],
      "source": [
        "train(3)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('deep-learning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "53ca6db60e8572b5cca1a2505e0e8f40644b2c5d709dfa1cf5346fd4aed1282b"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}